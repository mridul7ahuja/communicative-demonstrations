{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import random\n",
    "from ddt import ddt, data, unpack\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Algorithms/')\n",
    "import ValueIteration as targetCode\n",
    "sys.path.append('../Environments/')\n",
    "from ColoredGridWorld.MDP import MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...................\n",
      "----------------------------------------------------------------------\n",
      "Ran 19 tests in 0.052s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@ddt\n",
    "class TestValueIteration(unittest.TestCase):\n",
    "    def setUp(self): \n",
    " \n",
    "    # set epsilon to 0 and beta to something large, this becomes similar to the max problem of the orifinal\n",
    "    ## should add a test about the behavior at the terminal site\n",
    "    ## Add a test for each of the parameters (temp and rationality)\n",
    "    \n",
    "        self.convergenceTolerance = .000001\n",
    "        self.gamma = .9\n",
    "        self.eps = 0\n",
    "        self.alpha = 20\n",
    "    \n",
    "        dimensions = (3,3)\n",
    "        self.goalState = (2,2)\n",
    "        self.actions = [(-1,0),(0,1),(0,-1),(1,0)]\n",
    "        colourReward = {'green': 10, 'red': -100, 'white': 0}\n",
    "        stateSpace = {(0,0): 'white', (0,1): 'white', (0,2): 'white', (1,0): 'white', (1,1): 'white', (1,2):'white', (2,0): 'white', (2,1):'red', (2,2):'green'}\n",
    "        getMDP = MDP(dimensions, stateSpace, colourReward)\n",
    "        self.deterministicRewardFunction, self.deterministicTransitionFunction = getMDP()\n",
    "        self.deterministicValueTable = {key:0 for key in stateSpace.keys()}\n",
    "        \n",
    "        self.performDeterministicTransitionValueIteration = targetCode.ValueIteration(self.actions, self.deterministicTransitionFunction, self.deterministicRewardFunction, self.deterministicValueTable, self.goalState, self.convergenceTolerance, self.gamma, self.alpha, self.eps, True)\n",
    "\n",
    "\n",
    "    @data(((1,2), (1,1)), ((1,2), (0,2)), ((0,1), (0,0))) \n",
    "    @unpack\n",
    "    def test_relativeStateValues_DeterministicTransition_FirstStateGreaterValue(self, state1, state2, roundingTolerance = 5):\n",
    "        optimalValues, _ = self.performDeterministicTransitionValueIteration()\n",
    "\n",
    "        state1Value = round(optimalValues[state1], roundingTolerance)\n",
    "        state2Value = round(optimalValues[state2], roundingTolerance)\n",
    "\n",
    "        calculatedRelativeStateValueSign = np.sign(state1Value - state2Value)\n",
    "        expectedSign = 1\n",
    "        self.assertEqual(calculatedRelativeStateValueSign, expectedSign)\n",
    "    \n",
    "    @data(((1,1), (0,2)), ((0,0), (2,0)), ((0,1), (1,0)))\n",
    "    @unpack\n",
    "    def test_relativeStateValues_DeterministicTransition_EquivalentValueStates(self, state1, state2, roundingTolerance = 5):\n",
    "        optimalValues, _ = self.performDeterministicTransitionValueIteration()\n",
    "\n",
    "        state1Value = round(optimalValues[state1], roundingTolerance)\n",
    "        state2Value = round(optimalValues[state2], roundingTolerance)\n",
    "\n",
    "        calculatedRelativeStateValueSign = np.sign(state1Value - state2Value)\n",
    "        expectedSign = 0\n",
    "        self.assertEqual(calculatedRelativeStateValueSign, expectedSign)\n",
    "\n",
    "    @data(((2,0), (0,1)), ((1,1), (1,0)), ((2,1),(1,0)))\n",
    "    @unpack\n",
    "    def test_isActionNonzeroInProbability_ZeroProb_DeterministicTransition_ToTrapState(self, state, action):\n",
    "        _, policy = self.performDeterministicTransitionValueIteration()\n",
    "        probabilityToTrapState = policy[state][action]\n",
    "        roundedProbabilityToTrapState = round(probabilityToTrapState, 5)\n",
    "        self.assertEqual(probabilityToTrapState,0)\n",
    "\n",
    "    \n",
    "    @data(((0,0), (-1,0)), ((0,0),(0,-1)), ((0,1),(-1,0)))\n",
    "    @unpack\n",
    "    def test_isActionNonzeroInProbability_ZeroProb_DeterministicTransition_OffBoard(self, state, action):\n",
    "        _, policy = self.performDeterministicTransitionValueIteration()\n",
    "        probabilityOffBoard = policy[state][action]\n",
    "        roundedProbabilityOffBoard = round(probabilityOffBoard, 5)\n",
    "        self.assertEqual(roundedProbabilityOffBoard,0) \n",
    "\n",
    "    @data(((0,0), (1,0), (0,1)), ((0,1), (0,1), (0,1)))\n",
    "    @unpack\n",
    "    def test_nonVital_isActionNonzeroInProbability_NonzeroProb_DeterministicTransition_MultipleOptimalDirections(self, state, action1, action2):\n",
    "        _, policy = self.performDeterministicTransitionValueIteration()\n",
    "        probability1 = policy[state][action1]\n",
    "        probability1Rounded = round(probability1, 5)\n",
    "        probability2 = policy[state][action2]\n",
    "        probability2Rounded = round(probability2, 5)\n",
    "        self.assertEqual(probability1Rounded, probability2Rounded)\n",
    "      \n",
    "    #testing for rationality(non-value related randomness) by putting eps=1\n",
    "    @data((0,0), (1,0), (0,1), (2,1), (1,2))\n",
    "    def test_Rationality_UniformlyRandom(self, state):\n",
    "        performRandomValueIteration = targetCode.ValueIteration(self.actions, self.deterministicTransitionFunction, self.deterministicRewardFunction, self.deterministicValueTable, self.goalState, self.convergenceTolerance, self.gamma, self.alpha, 1, True)\n",
    "        _, policy = performRandomValueIteration()\n",
    "        probabilityOfAnyAction = random.choice(list(policy[state].values()))\n",
    "        uniformProbability = 1/len(self.actions)\n",
    "        self.assertEqual(probabilityOfAnyAction, uniformProbability)\n",
    "    \n",
    "    #testing for softmax temp by putting alpha = 0     \n",
    "    @data((0,0), (1,0), (0,1), (2,1))\n",
    "    def test_Rationality_UniformlyRandom(self, state):\n",
    "        performRandomValueIteration = targetCode.ValueIteration(self.actions, self.deterministicTransitionFunction, self.deterministicRewardFunction, self.deterministicValueTable, self.goalState, self.convergenceTolerance, self.gamma, 0, self.eps, True)\n",
    "        _, policy = performRandomValueIteration()\n",
    "        probabilityOfAnyAction = random.choice(list(policy[state].values()))\n",
    "        uniformProbability = 1/len(self.actions)\n",
    "        self.assertEqual(probabilityOfAnyAction, uniformProbability)\n",
    "        \n",
    "    def test_TerminalStateValue(self):\n",
    "        valueTable,_ = self.performDeterministicTransitionValueIteration()\n",
    "        TerminalStateValue = valueTable[self.goalState]\n",
    "        initializedValue = 0\n",
    "        self.assertEqual(TerminalStateValue, initializedValue)\n",
    "    \n",
    "    def tearDown(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
