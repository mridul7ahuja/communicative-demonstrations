{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import functools \n",
    "from itertools import product\n",
    "import math\n",
    "from BuildWorldClass import buildWorld \n",
    "from BuildMDPClass import buildMDP\n",
    "from ValueIterationWithSoftmax import ValueIteration\n",
    "from ActionInterpretationClass import actionInterpretation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of the given world and goal for trajectory 1 is 0.0038231297086904984\n",
      "Probability of the given world and goal for trajectory 2 is 0.9999688091347915\n"
     ]
    }
   ],
   "source": [
    "def buildUtilitySpace(variableColours, variableReward, constantRewardDict):\n",
    "    utilitySpace = [{key:value for key, value in zip(variableColours, permutations)} for permutations in product(variableReward, repeat = len(variableColours))]\n",
    "    for i in range(len(utilitySpace)):\n",
    "        utilitySpace[i].update(constantRewardDict)\n",
    "    return utilitySpace\n",
    "\n",
    "def buildPoliciesAndMDPs(dimensions, stateSpace, utilitySpace, beliefSpace, actions, valueTable, hyperparameters):\n",
    "    convergenceTolerance, gamma, alpha, eps = hyperparameters \n",
    "    beliefMDPs = [buildMDP(dimensions, stateSpace, colourReward) for colourReward in utilitySpace] \n",
    "    rewardAndTransitionFunctions = [MDP() for MDP in beliefMDPs]\n",
    "    ValueIterations = [ValueIteration(actions, transitionPdf, rewardFunction, valueTable, convergenceTolerance, gamma, alpha, eps) for rewardFunction, transitionPdf in rewardAndTransitionFunctions]\n",
    "    ValueAndPolicyTables = [ValueIteration() for ValueIteration in ValueIterations]\n",
    "    beliefPolicies = [ValueAndPolicyTable[1] for ValueAndPolicyTable in ValueAndPolicyTables]\n",
    "    return {(belief):(MDP,policy) for belief, MDP, policy in zip(beliefSpace, beliefMDPs, beliefPolicies)}\n",
    "\n",
    "def buildWorlds(utilitySpace, transitionSpace):\n",
    "    return [buildWorld(colourReward, isDeterministic) for colourReward, isDeterministic in product(utilitySpace, transitionSpace)]\n",
    "            \n",
    "def main():\n",
    "    dimensions = (5,6)\n",
    "    goals = [(5,2)]\n",
    "    actions = {(-1,0),(0,1),(0,-1),(1,0)}\n",
    "    goalNameDictionary = {(5,2):'goal'}\n",
    "    stateSpace = {(0,0): 'white',(0,1): 'white',(0,2): 'white',(0,3): 'white',(0,4): 'white', (1,0): 'blue',(1,1): 'orange', (1,2):'orange',(1,3):'orange',(1,4):'orange', (2,0): 'blue',(2,1):'purple', (2,2):'purple', (2,3):'purple', (2,4):'orange', (3,0): 'blue',(3,1):'purple',(3,2): 'blue',(3,3):'purple',(3,4):'orange', (4,0): 'blue', (4,1): 'blue', (4,2): 'blue',(4,3):'purple', (4,4):'orange', (5,0):'white',(5,1):'white', (5,2):'yellow', (5,3):'white', (5,4):'white'}\n",
    "    convergenceTolerance = 10e-7\n",
    "    gamma = 0.94\n",
    "    alpha = 20\n",
    "    eps = 0.01\n",
    "    hyperparameters = (convergenceTolerance, gamma, alpha, eps)\n",
    "    valueTable = {key: 0 for key in stateSpace.keys()}\n",
    "    colourReward = {'white': 0, 'orange': -2, 'purple': 0, 'blue':0, 'yellow': 10}\n",
    "    variableColours = ['orange', 'purple', 'blue']\n",
    "    variableReward = [0, -2]\n",
    "    constantRewardDict = {'white': 0, 'yellow': 10}\n",
    "    \n",
    "    \n",
    "    utilitySpace = buildUtilitySpace(variableColours, variableReward, constantRewardDict)\n",
    "    transitionSpace = [True]\n",
    "    worlds = buildWorlds(utilitySpace, transitionSpace)\n",
    "    beliefSpace = [(world, goal) for world, goal in product(worlds, goals)]\n",
    "    beliefPoliciesAndMDPs = buildPoliciesAndMDPs(dimensions, stateSpace, utilitySpace, beliefSpace, actions, valueTable, hyperparameters)\n",
    "    priors = {(belief): (1/len(beliefSpace)) for belief in beliefSpace}\n",
    "    \n",
    "    \n",
    "    beliefVector = actionInterpretation(beliefPoliciesAndMDPs, priors)  \n",
    "    trajectory = [(0,2), (1,0), (1,2), (1,0), (2,2), (1,0), (3,2), (1,0), (4,2), (1,0), (5,2)]\n",
    "    trajectory2 = [(0,2), (0,-1), (0,1), (0,-1), (0,0), (1,0), (1,0), (1,0), (2,0), (0,1), (2,1), (0,1), (2,2),(1,0), (3,2), (1,0), (4,2), (1,0), (5,2)]\n",
    "    testWorld = buildWorld({'white': 0, 'orange': -2, 'purple': 0, 'blue':0, 'yellow': 10} ,True)\n",
    "    Probab = beliefVector(trajectory, testWorld, goals[0])\n",
    "    Probab2 = beliefVector(trajectory2, testWorld, goals[0])\n",
    "    print(\"Probability of the given world and goal for trajectory 1 is \" + str(Probab))\n",
    "    print(\"Probability of the given world and goal for trajectory 2 is \" + str(Probab2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
