{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import functools \n",
    "from itertools import product\n",
    "import sys\n",
    "sys.path.append('../Environments/')\n",
    "from ColoredGridWorld import SetUpInferenceSpace as setUp\n",
    "from ColoredGridWorld.MDP import MDP\n",
    "from ColoredGridWorld import OBMDP\n",
    "from ColoredGridWorld import visualizations\n",
    "sys.path.append('../Algorithms/')\n",
    "from ActionInterpretation import ActionInterpretation\n",
    "from ValueIteration import ValueIteration\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJointTrajectory(jointState, policyTable, goal, jointTransitionFn):\n",
    "    traj = [jointState[0]]\n",
    "    while(jointState[0]!=goal):\n",
    "        action = max(policyTable[jointState], key=policyTable[jointState].get)\n",
    "        jointState = max(jointTransitionFn(jointState, action), key = jointTransitionFn(jointState, action).get)\n",
    "        traj.append(jointState[0])\n",
    "    return traj\n",
    "        \n",
    "       \n",
    "def viewPolicyStructure(d, levels, indent=0):\n",
    "    for key, value in d.items():\n",
    "        if (indent==0):\n",
    "            beliefDict = key[1]()\n",
    "            strkey = \"(\"\n",
    "            for env, prob in beliefDict.items():\n",
    "                strkey = strkey + str(env[0]()) + \", \" + str(env[1]) + \") :\" + str(prob) + \", \"\n",
    "            strkey = strkey[:len(strkey)-2] + strkey[len(strkey):]\n",
    "            strkey = strkey + \")\"\n",
    "            print('\\t' * indent + str(levels[indent]) + \": \"+ str(key[0]) + strkey)\n",
    "        else: \n",
    "            print('\\t' * indent + str(levels[indent]) + \": \"+ str(key))\n",
    "        if isinstance(value, dict):\n",
    "            viewPolicyStructure(value, levels, indent+1)\n",
    "        else:\n",
    "            print('\\t' * (indent+1) + str(levels[indent+1])+ \": \" + str(value))\n",
    "\n",
    "def main():\n",
    "    dimensions = (5,6)\n",
    "    goals = [(5,2)]\n",
    "    goalState = (5,2)\n",
    "    actions = {(-1,0),(0,1),(0,-1),(1,0)}\n",
    "    goalNameDictionary = {(5,2):'goal'}\n",
    "    colourReward = {'white': 0, 'orange': -2, 'purple': 0, 'blue':0, 'yellow':10}\n",
    "    stateSpace = {(0,0): 'white',(0,1): 'white',(0,2): 'white',(0,3): 'white',(0,4): 'white', (1,0): 'blue',(1,1): 'orange', (1,2):'orange',(1,3):'orange',(1,4):'orange', (2,0): 'blue',(2,1):'purple', (2,2):'purple', (2,3):'purple', (2,4):'orange', (3,0): 'blue',(3,1):'purple',(3,2): 'blue',(3,3):'purple',(3,4):'orange', (4,0): 'blue', (4,1): 'blue', (4,2): 'blue',(4,3):'purple', (4,4):'orange', (5,0):'white',(5,1):'white', (5,2):'yellow', (5,3):'white', (5,4):'white'}\n",
    "    getMDP = MDP(dimensions, stateSpace, colourReward)\n",
    "    objectRewardFn, objectTransitionFn = getMDP()\n",
    "    \n",
    "    #set up for value-iteration and inference\n",
    "    convergenceTolerance = 10e-4\n",
    "    gamma = 0.95\n",
    "    alpha = 20\n",
    "    eps = 0.05\n",
    "    hyperparameters = (convergenceTolerance, gamma, alpha, eps)\n",
    "    variableColours = ['orange', 'purple', 'blue']\n",
    "    variableReward = [0, -2]\n",
    "    constantRewardDict = {'white': 0, 'yellow': 10}\n",
    "    \n",
    "    \n",
    "    utilitySpace = setUp.buildUtilitySpace(variableColours, variableReward, constantRewardDict)\n",
    "    transitionSpace = [True]\n",
    "    worlds = setUp.buildWorldSpace(utilitySpace, transitionSpace)\n",
    "    envSpace = [(world, goal) for world, goal in product(worlds, goals)]\n",
    "    envMDPsAndPolicies = setUp.buildEnvPolicySpace(dimensions, stateSpace, actions, envSpace, hyperparameters)\n",
    "    actionInterpretation = ActionInterpretation(envMDPsAndPolicies)\n",
    "    \n",
    "    #set up for OBMDP and its value iteration \n",
    "    beta = 10\n",
    "    beliefGamma = 0.95\n",
    "    beliefAlpha = 20\n",
    "    beliefEps = 0.05\n",
    "    bins = [0,0.25,0.5,0.75,1]\n",
    "    beliefSpacePossible = [{key:value for key, value in zip(envSpace, permutations)} for permutations in product(bins, repeat = len(envSpace))]\n",
    "    discreteBeliefSpace = [beliefDict for beliefDict in beliefSpacePossible if (sum(value for value in beliefDict.values())==1)]\n",
    "    hashableDiscreteBeliefSpace = [setUp.HashableBelief(beliefDict) for beliefDict in discreteBeliefSpace]\n",
    "    beliefUtilityFn = OBMDP.getBeliefUtility()\n",
    "    literalObserver = OBMDP.LiteralObserver(actionInterpretation)\n",
    "    getNextBelief = literalObserver(discreteBeliefSpace, True)\n",
    "    jointStateSpace = list(product(stateSpace.keys(), hashableDiscreteBeliefSpace))\n",
    "    getOBMDP = OBMDP.OBMDP(jointStateSpace, (setUp.HashableWorld(colourReward, True), goals[0]), True, beta)\n",
    "    jointRewardFn, jointTransitionFn = getOBMDP(objectTransitionFn, objectRewardFn, getNextBelief, beliefUtilityFn)\n",
    "    \n",
    "    #implementing object-level MDP\n",
    "    valueTable = {key: 0 for key in stateSpace.keys()}\n",
    "    performValueIteration = ValueIteration(actions, objectTransitionFn, objectRewardFn, valueTable, [goalState], convergenceTolerance, gamma, alpha, eps, True)\n",
    "    optimalValues, policyTable = performValueIteration()\n",
    "    trapStates = [s for s in stateSpace if(colourReward[stateSpace[s]]<0)]\n",
    "    m,n = dimensions \n",
    "    visualizations.visualizeValueTable(m, n, goalState, trapStates, optimalValues)\n",
    "    visualizations.visualizePolicy(stateSpace, policyTable, goalState, otherGoals=[], trapStates=[], arrowScale = .3)\n",
    "    \n",
    "    #implementing OBMDP\n",
    "    valueTable = {key: 0 for key in jointStateSpace}\n",
    "    jointGoalStates = [jointState for jointState in jointStateSpace if jointState[0] == goalState]\n",
    "    performValueIteration = ValueIteration(actions, jointTransitionFn, jointRewardFn, valueTable, jointGoalStates, convergenceTolerance, beliefGamma, beliefAlpha, beliefEps, True)\n",
    "    optimalValues, policyTable = performValueIteration()\n",
    "    initialState1 = ( (0,2), setUp.HashableBelief({(setUp.HashableWorld({'orange':0, 'white':0, 'blue':0, 'purple':0, 'yellow':10}, True),(5,2)): 0.25, \n",
    "                                                   (setUp.HashableWorld({'orange':0, 'white':0, 'blue':0, 'purple':-2, 'yellow':10}, True),(5,2)): 0, \n",
    "                                                   (setUp.HashableWorld({'orange':0, 'white':0, 'blue':-2, 'purple':0, 'yellow':10}, True),(5,2)): 0.25, \n",
    "                                                   (setUp.HashableWorld({'orange':0, 'white':0, 'blue':-2, 'purple':-2, 'yellow':10}, True),(5,2)): 0,\n",
    "                                                   (setUp.HashableWorld({'orange':-2, 'white':0, 'blue':0, 'purple':0, 'yellow':10}, True),(5,2)): 0.25,\n",
    "                                                   (setUp.HashableWorld({'orange':-2, 'white':0, 'blue':0, 'purple':-2, 'yellow':10}, True),(5,2)): 0,\n",
    "                                                   (setUp.HashableWorld({'orange':-2, 'white':0, 'blue':-2, 'purple':0, 'yellow':10}, True),(5,2)): 0.25,\n",
    "                                                   (setUp.HashableWorld({'orange':-2, 'white':0, 'blue':-2, 'purple':-2, 'yellow':10}, True),(5,2)): 0,\n",
    "                                                   }) )\n",
    "    trajectory1 = getJointTrajectory(initialState1, policyTable, goalState, jointTransitionFn)\n",
    "    #viewPolicyStructure(policyTable, [\"state\", \"action\", \"probability\"])\n",
    "    visualizations.visualizeEnvironmentByState(stateSpace, [goalState], [], trajectory1, goalNameDictionary, 2.5)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
